{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize 3d conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First results:   \n",
    "<pre>\n",
    "python perf_convs.py --inputs 20 20 20 20 3 --filters 7 3 6 2 3      \n",
    "Input shape  [20, 20, 20, 20, 3]\n",
    "Filter shape [7, 3, 6, 2, 3]\n",
    "Output shape (20, 18, 15, 19, 7)\n",
    "#Inputs   480000\n",
    "#Weights     756\n",
    "#Outputs  718200\n",
    "#Multiplications 542959200\n",
    "Theano 3d                 runtime per iteration (  14 iterations): 143.8815ms\n",
    "Theano 3d2d               runtime per iteration (  78 iterations):  12.9532ms\n",
    "Python Vectorized         runtime per iteration (  25 iterations):  82.0083ms\n",
    "Cudamat Vectorized        runtime per iteration (  23 iterations):  87.3997ms\n",
    "Theano Mul Vectorized     runtime per iteration (  28 iterations):  72.9542ms\n",
    "GPU Loop                  runtime per iteration (   1 iterations): 20829.8290ms\n",
    "\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Cudnn theano 3d2d faster:\n",
    "\n",
    "<pre>\n",
    "python perf_convs.py --inputs 20 20 20 20 3 --filters 7 3 6 2 3\n",
    "Using gpu device 0: GeForce GTX 780\n",
    "Batches/Filters, rows, columns, times, channels\n",
    "Input shape  [20, 20, 20, 20, 3]\n",
    "Filter shape [7, 3, 6, 2, 3]\n",
    "Output shape (20, 18, 15, 19, 7)\n",
    "#Inputs   480000\n",
    "#Weights     756\n",
    "#Outputs  718200\n",
    "#Multiplications 542959200\n",
    "\n",
    "Theano 3d                 runtime per iteration (  14 iterations): 143.8013ms\n",
    "Theano 3d2d               runtime per iteration ( 286 iterations):   3.5014ms\n",
    "Python Vectorized         runtime per iteration (  25 iterations):  80.9831ms\n",
    "Cudamat Vectorized        runtime per iteration (  24 iterations):  86.5586ms\n",
    "Theano Mul Vectorized     runtime per iteration (  28 iterations):  71.6942ms\n",
    "GPU Loop                  runtime per iteration (   1 iterations): 20715.0171ms\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 80 x 80 x 3 image input with 5 - 40 frames batch size 64, 256\n",
    "# 5 x  5 x 3 x 5(zeit) filter, 128 davon\n",
    "# wenn im vergleich yuzu 2d nur um faktor zeit oder ein bisschen mehr groesser dann ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wrap as a layer usable in theano:  https://github.com/benanne/Lasagne/blob/master/lasagne/layers/corrmm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or pylearn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum of batches * filters: 3872\n",
      "('32x32?', True)\n",
      "('32x64?', True)\n",
      "('64x64?', False)\n"
     ]
    }
   ],
   "source": [
    "# For given inputs above we can have maximum (filtersxbatchsize)\n",
    "# http://www.nvidia.de/object/geforce-gtx-780-de.html#pdpContent=2 => 3072 MB RAM\n",
    "# Keep in mind this is ignoring memory for filters and input and assuming all memory is free before convolution\n",
    "\n",
    "GPU_memory_bytes = 3072 * (1024**2)\n",
    "given_output_elements = ((80 - 5 + 1) * (80 - 5 + 1) * (40 - 5 + 1))\n",
    "float_bytes = 4\n",
    "\n",
    "maximum_batches_times_filters = GPU_memory_bytes / (given_output_elements * float_bytes)\n",
    "print \"Maximum of batches * filters:\", maximum_batches_times_filters\n",
    "print(\"32x32?\", 32*32 < maximum_batches_times_filters)\n",
    "print(\"32x64?\", 32*64 < maximum_batches_times_filters)\n",
    "print(\"64x64?\", 64*64 < maximum_batches_times_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/schirrmr/3dconv/pylearn3dconv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.sys.path.insert(0,'/home/schirrmr/3dconv/') \n",
    "%cd /home/schirrmr/3dconv/pylearn3dconv/\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780\n",
      "inside conv fwd\n",
      "returning zero from set tensor\n",
      "returning zero from set filter\n",
      "after conv fwd setting tensors/filters\n",
      "returning zero from set tensor\n",
      "Handle zu 119899872\n",
      "Handle d 119899872\n",
      "Conv algo 3\n",
      "Input descriptor\n",
      "Float: 1\n",
      "nbDims: 5\n",
      "Dimensions: 3 3 3 3 3 \n",
      "Strides: 81 27 9 3 1 \n",
      "\n",
      "Filter descriptor\n",
      "Float: 1\n",
      "nbDims: 5\n",
      "Dimensions: 3 3 3 3 3 \n",
      "\n",
      "Conv descriptor\n",
      "Float: 1\n",
      "conv dims: 3\n",
      "Paddings: 0 0 0 \n",
      "filterStrideA: 1 1 1 \n",
      "upscaleA: 1 1 1 \n",
      "mode convolution: 1\n",
      "mode correlation: 0\n",
      "\n",
      "Output\n",
      "Float: 1\n",
      "nbDims: 5\n",
      "Dimensions: 3 3 1 1 1 \n",
      "Strides: 3 1 1 1 1 \n",
      "after cudnnGetTensorNdDescriptor\n",
      "handle 94159552\n",
      "after cudnnGetConvolutionForwardWorkspaceSize\n",
      "worksize requested: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"../ipython_scripts.py\", line 82, in <module>\n",
      "    result=conv_result_func(real_inputs, real_filters)\n",
      "  File \"/home/schirrmr/3dconv/Theano/theano/compile/function_module.py\", line 597, in __call__\n",
      "    outputs = self.fn()\n",
      "  File \"/home/schirrmr/3dconv/Theano/theano/compile/debugmode.py\", line 2144, in deco\n",
      "    return f()\n",
      "  File \"/home/schirrmr/3dconv/Theano/theano/compile/debugmode.py\", line 1989, in f\n",
      "    (exc_type, exc_value, exc_trace))\n",
      "  File \"/home/schirrmr/3dconv/Theano/theano/compile/debugmode.py\", line 1969, in f\n",
      "    thunk_c()\n",
      "  File \"/home/schirrmr/3dconv/Theano/theano/gof/cc.py\", line 1545, in __call__\n",
      "    raise exc_type, exc_value, exc_trace\n",
      "RuntimeError: An optimization (probably ('Revert', 'output_guard') ) inserted an apply node that raise an error.\n",
      "The information we have about this optimizations is:OutputGuard.0\n",
      "  OutputGuard [@A] ''   \n",
      "   |GpuDnn3dConv{workmem='small'} [@B] ''   \n",
      "     |<CudaNdarrayType(float32, 5D)> [@C]\n",
      "     |<CudaNdarrayType(float32, 5D)> [@D]\n",
      "     |GpuAllocEmpty [@E] ''   \n",
      "     | |Shape_i{0} [@F] ''   \n",
      "     | | |<CudaNdarrayType(float32, 5D)> [@C]\n",
      "     | |Shape_i{0} [@G] ''   \n",
      "     | | |<CudaNdarrayType(float32, 5D)> [@D]\n",
      "     | |Elemwise{Composite{(i0 + ((i1 - i2) // i0))}} [@H] ''   \n",
      "     | | |TensorConstant{1} [@I]\n",
      "     | | |Shape_i{2} [@J] ''   \n",
      "     | | | |<CudaNdarrayType(float32, 5D)> [@C]\n",
      "     | | |Shape_i{2} [@K] ''   \n",
      "     | |   |<CudaNdarrayType(float32, 5D)> [@D]\n",
      "     | |Elemwise{Composite{(i0 + ((i1 - i2) // i0))}} [@L] ''   \n",
      "     | | |TensorConstant{1} [@I]\n",
      "     | | |Shape_i{3} [@M] ''   \n",
      "     | | | |<CudaNdarrayType(float32, 5D)> [@C]\n",
      "     | | |Shape_i{3} [@N] ''   \n",
      "     | |   |<CudaNdarrayType(float32, 5D)> [@D]\n",
      "     | |Elemwise{Composite{(i0 + ((i1 - i2) // i0))}} [@O] ''   \n",
      "     |   |TensorConstant{1} [@I]\n",
      "     |   |Shape_i{4} [@P] ''   \n",
      "     |   | |<CudaNdarrayType(float32, 5D)> [@C]\n",
      "     |   |Shape_i{4} [@Q] ''   \n",
      "     |     |<CudaNdarrayType(float32, 5D)> [@D]\n",
      "     |GpuDnnConv3dDesc{subsample=(1, 1, 1), conv_mode='conv'} [@R] ''   \n",
      "     |Constant{1.0} [@S]\n",
      "     |Constant{0.0} [@T]\n",
      "\n",
      "\n",
      "The original exception: \n",
      "GpuDnn3dConv: error getting worksize: CUDNN_STATUS_NOT_SUPPORTED\n",
      "Apply node that caused the error: GpuDnn3dConv{workmem='small'}(<CudaNdarrayType(float32, 5D)>, <CudaNdarrayType(float32, 5D)>, GpuAllocEmpty.0, GpuDnnConv3dDesc{subsample=(1, 1, 1), conv_mode='conv'}.0, Constant{1.0}, Constant{0.0})\n",
      "Inputs types: [CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, 5D), <theano.gof.type.CDataType object at 0x7f5089ffe8d0>, Scalar(float32), Scalar(float32)]\n",
      "Inputs shapes: [(3, 3, 3, 3, 3), (3, 3, 3, 3, 3), (3, 3, 1, 1, 1), 'No shapes', (), ()]\n",
      "Inputs strides: [(81, 27, 9, 3, 1), (81, 27, 9, 3, 1), (3, 1, 0, 0, 0), 'No strides', (), ()]\n",
      "Inputs values: ['not shown', 'not shown', 'not shown', <PyCObject object at 0x7f50880be1e8>, 1.0, 0.0]\n",
      "\n",
      "Backtrace when the node is created:\n",
      "  File \"/home/schirrmr/3dconv/pylearn3dconv/theano_dnn_first_try/theano_dnn_conv.py\", line 225, in make_node\n",
      "    [output.type()])\n",
      "\n",
      "HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\n"
     ]
    }
   ],
   "source": [
    "#!PYTHONPATH=$PYTHONPATH:`pwd`/../ python ipython_scripts.py\n",
    "!PYTHONPATH=$PYTHONPATH:`pwd`/../../ python ../ipython_scripts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make test case with this data, check that you always get same results\n",
    "#\n",
    "# for pyyaml later:\n",
    "# create h5 files with data, create \"h5volumetricdataset\" class which reads out adata and calls superconstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: *** No rule to make target `volumetric_in_c'.  Stop.\r\n"
     ]
    }
   ],
   "source": [
    "!make volumetric_in_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: ./volumetric_in_c: not found\r\n"
     ]
    }
   ],
   "source": [
    "! ./volumetric_in_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN Black\n"
     ]
    }
   ],
   "source": [
    "from pylearn3dconv.volumetric_space import Conv3DSpace\n",
    "from pylearn3dconv.layers.theano_3d_conv import Theano3dConv3dElemwise\n",
    "import numpy as np\n",
    "from pylearn2.models.mlp import IdentityConvNonlinearity\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from pylearn3dconv.volumetric_dense_design_matrix import VolumetricDenseDesignMatrix\n",
    "from pylearn2.training_algorithms.sgd import SGD\n",
    "from pylearn2.models.mlp import MLP, Softmax, ConvElemwise\n",
    "from pylearn2.format.target_format import OneHotFormatter\n",
    "from numpy.random import RandomState\n",
    "from pylearn2.space import Conv2DSpace\n",
    "from pylearn3dconv.layers.blas2d_manuel_conv import ConvElemwiseBlas\n",
    "from pylearn3dconv.layers.cublas_3d_conv import CuBlasConv3dElemwise\n",
    "from pylearn3dconv.layers.cudnn_3d_conv import CuDnnConv3dElemwise\n",
    "from pylearn3dconv.perf.perf_layers import create_fprop_layer_3d_symbolic\n",
    "from pylearn3dconv.test_data import generate_test_data\n",
    "import theano.sandbox.cuda\n",
    "import theano\n",
    "import theano.sandbox.cuda.dnn as cdnn\n",
    "from theano.sandbox.cuda.basic_ops import (as_cuda_ndarray_variable,\n",
    "                                           host_from_gpu,\n",
    "                                           gpu_contiguous, HostFromGpu,\n",
    "                                           gpu_alloc_empty)\n",
    "from theano.sandbox.cuda.dnn import GpuDnnConv, GpuDnnConvDesc\n",
    "from numpy.random import RandomState\n",
    "from pylearn3dconv.theano_dnn_first_try.theano_dnn_conv import GpuDnn3dConv, GpuDnnConv3dDesc\n",
    "\n",
    "import numpy as np\n",
    "import theano_dnn_first_try.theano_dnn_conv as owndnn\n",
    "ftensor5 = T.TensorType('float32', (False,)*5)\n",
    "class FakeMLP():\n",
    "    def __init__(self,rng,batch_size):\n",
    "        self.rng = rng\n",
    "        self.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/schirrmr/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /home/schirrmr/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "from pylearn3dconv.theano_dnn_first_try.theano_dnn_conv import GpuDnnPool3dDesc, GpuDnn3dPool\n",
    "inputs_shape = [5,3,4,7,2]\n",
    "inputs_shape = [5,3,4,8,2]\n",
    "\n",
    "pool_shape = [4,8,2]\n",
    "pool_shape = [2,2,1]\n",
    "pool_stride = (2,7,1)\n",
    "pool_input = ftensor5()\n",
    "pool_desc = GpuDnnPool3dDesc(tuple(pool_shape),pool_stride, 'max', pad=(0,0,0))()\n",
    "dnn_3d_pool_result = GpuDnn3dPool()(pool_input, pool_desc)\n",
    "pool_func = theano.function([pool_input], dnn_3d_pool_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "GpuDnnPool: error doing cudnnPoolingForward operation: CUDNN_STATUS_NOT_SUPPORTED\nApply node that caused the error: GpuDnn3dPool(GpuFromHost.0, GpuDnnPool3dDesc{ws=(2, 2, 1), stride=(2, 7, 1), mode='max', pad=(0, 0, 0)}.0)\nInputs types: [CudaNdarrayType(float32, 5D), <theano.gof.type.CDataType object at 0x7f0ab292dfd0>]\nInputs shapes: [(5, 3, 4, 8, 2), 'No shapes']\nInputs strides: [(192, 64, 16, 2, 1), 'No strides']\nInputs values: ['not shown', <PyCObject object at 0x7f0ac9dd8d00>]\n\nBacktrace when the node is created:\n  File \"/home/schirrmr/3dconv/pylearn3dconv/theano_dnn_first_try/theano_dnn_conv.py\", line 634, in make_node\n    [img.type()])\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-bb0088409461>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpool_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/schirrmr/3dconv/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    606\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    609\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/schirrmr/3dconv/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: GpuDnnPool: error doing cudnnPoolingForward operation: CUDNN_STATUS_NOT_SUPPORTED\nApply node that caused the error: GpuDnn3dPool(GpuFromHost.0, GpuDnnPool3dDesc{ws=(2, 2, 1), stride=(2, 7, 1), mode='max', pad=(0, 0, 0)}.0)\nInputs types: [CudaNdarrayType(float32, 5D), <theano.gof.type.CDataType object at 0x7f0ab292dfd0>]\nInputs shapes: [(5, 3, 4, 8, 2), 'No shapes']\nInputs strides: [(192, 64, 16, 2, 1), 'No strides']\nInputs values: ['not shown', <PyCObject object at 0x7f0ac9dd8d00>]\n\nBacktrace when the node is created:\n  File \"/home/schirrmr/3dconv/pylearn3dconv/theano_dnn_first_try/theano_dnn_conv.py\", line 634, in make_node\n    [img.type()])\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "inputs = np.ones(inputs_shape).astype(np.float32)\n",
    "pool_func(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient for Cudnn Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Result from Cublas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get small input\n",
    "rng = RandomState(np.uint32(hash('tobiderpuma')))\n",
    "inputs_shape = [5,8,4,7,3]\n",
    "inputs_shape = [5,2,4,7,3]\n",
    "filters_shape = [6,2,3,5,3]\n",
    "\n",
    "bias *= 0\n",
    "\n",
    "inputs, filters, bias = generate_test_data(rng, inputs_shape, filters_shape)\n",
    "# compute gradient for Cublas\n",
    "# do it twice, compare result\n",
    "x = T.dscalar('x')\n",
    "inputs_theano = ftensor5()\n",
    "\n",
    "conv_result = create_fprop_layer_3d_symbolic(inputs_shape, filters, bias, CuBlasConv3dElemwise, inputs_theano)\n",
    "cost = T.sum(conv_result)\n",
    "conv_gradient = T.grad(cost, inputs_theano)\n",
    "grad_func = theano.function([inputs_theano], conv_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_result = grad_func(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result from manually using Cudnn Op - fine, same result as Cublas if bias is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/schirrmr/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /home/schirrmr/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "inputs_theano_cudnn_op = ftensor5()\n",
    "filters_theano_cudnn_op = ftensor5()\n",
    "\n",
    "x  = gpu_contiguous(inputs_theano_cudnn_op)\n",
    "filters_cudnn_op = gpu_contiguous(filters_theano_cudnn_op)\n",
    "desc = GpuDnnConv3dDesc(conv_mode='cross')()\n",
    "desc_op = desc.owner.op\n",
    "out_shp = GpuDnn3dConv.get_out_shape(x.shape, filters_cudnn_op.shape,\n",
    "                                   desc_op.subsample)\n",
    "\n",
    "\n",
    "out = gpu_alloc_empty(*out_shp)\n",
    "rval = GpuDnn3dConv()(x, filters_cudnn_op, out, desc)\n",
    "cost_cudnn_op = T.sum(rval)\n",
    "conv_dnn_op_gradient = T.grad(cost_cudnn_op, inputs_theano_cudnn_op)\n",
    "grad_func_cudnn_op = theano.function([inputs_theano_cudnn_op, filters_cudnn_op], conv_dnn_op_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "GpuDnn3dConvGradI: error doing operation: CUDNN_STATUS_BAD_PARAM\nApply node that caused the error: GpuDnn3dConvGradI{inplace=False}(<CudaNdarrayType(float32, 5D)>, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConv3dDesc{subsample=(1, 1, 1), conv_mode='cross'}.0, Constant{1.0}, Constant{0.0})\nInputs types: [CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, 5D), <theano.gof.type.CDataType object at 0x7feac419a550>, Scalar(float32), Scalar(float32)]\nInputs shapes: [(6, 2, 3, 5, 3), (5, 6, 2, 3, 1), (5, 8, 4, 7, 3), 'No shapes', (), ()]\nInputs strides: [(90, 45, 15, 3, 1), (36, 6, 3, 1, 0), (672, 84, 21, 3, 1), 'No strides', (), ()]\nInputs values: ['not shown', 'not shown', 'not shown', <PyCObject object at 0x7feac48a82b0>, 1.0, 0.0]\n\nBacktrace when the node is created:\n  File \"/home/schirrmr/3dconv/pylearn3dconv/theano_dnn_first_try/theano_dnn_conv.py\", line 457, in make_node\n    def infer_shape(self, node, shape):\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-1820ce210406>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcudnn_op_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_func_cudnn_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcudnn_op_result\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcorrect_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/schirrmr/3dconv/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    606\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    609\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/schirrmr/3dconv/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: GpuDnn3dConvGradI: error doing operation: CUDNN_STATUS_BAD_PARAM\nApply node that caused the error: GpuDnn3dConvGradI{inplace=False}(<CudaNdarrayType(float32, 5D)>, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConv3dDesc{subsample=(1, 1, 1), conv_mode='cross'}.0, Constant{1.0}, Constant{0.0})\nInputs types: [CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, 5D), <theano.gof.type.CDataType object at 0x7feac419a550>, Scalar(float32), Scalar(float32)]\nInputs shapes: [(6, 2, 3, 5, 3), (5, 6, 2, 3, 1), (5, 8, 4, 7, 3), 'No shapes', (), ()]\nInputs strides: [(90, 45, 15, 3, 1), (36, 6, 3, 1, 0), (672, 84, 21, 3, 1), 'No strides', (), ()]\nInputs values: ['not shown', 'not shown', 'not shown', <PyCObject object at 0x7feac48a82b0>, 1.0, 0.0]\n\nBacktrace when the node is created:\n  File \"/home/schirrmr/3dconv/pylearn3dconv/theano_dnn_first_try/theano_dnn_conv.py\", line 457, in make_node\n    def infer_shape(self, node, shape):\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "cudnn_op_result = grad_func_cudnn_op(inputs, filters)\n",
    "assert np.sum(np.square(cudnn_op_result - correct_result)) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Result from using 2d dnn op - fine, same as reference if bias is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/schirrmr/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /home/schirrmr/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "inputs_theano_dnn2d = T.ftensor4()\n",
    "filters_theano_dnn2d = T.ftensor4()\n",
    "\n",
    "x  = gpu_contiguous(inputs_theano_dnn2d)\n",
    "filters_theano_dnn2d = gpu_contiguous(filters_theano_dnn2d)\n",
    "desc = GpuDnnConvDesc(border_mode='valid', conv_mode='cross')(x.shape, filters_theano_dnn2d.shape)\n",
    "desc_op = desc.owner.op\n",
    "out_shp = GpuDnnConv.get_out_shape(x.shape, filters_theano_dnn2d.shape, 'valid',\n",
    "                                   desc_op.subsample)\n",
    "\n",
    "\n",
    "out = gpu_alloc_empty(*out_shp)\n",
    "rval = GpuDnnConv()(x, filters_theano_dnn2d, out, desc)\n",
    "cost_dnn2d_op = T.sum(rval)\n",
    "conv_dnn2d_op_gradient = T.grad(cost_dnn2d_op, inputs_theano_dnn2d)\n",
    "grad_func_dnn2d_op = theano.function([inputs_theano_dnn2d, filters_theano_dnn2d], conv_dnn2d_op_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "dnn2d_result = grad_func_dnn2d_op(inputs[:,:,:,0,:], filters[:,:,:,0,:])\n",
    "print np.sum(np.square(dnn2d_result - correct_result[:,:,:,0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer - all fine same as cublas layer unless 0 dimension filter and input shape is equal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/schirrmr/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /home/schirrmr/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "inputs_theano_cudnn = ftensor5()\n",
    "inputs_theano_cudnn_contiguous = gpu_contiguous(inputs_theano_cudnn)\n",
    "conv_result_cudnn = create_fprop_layer_3d_symbolic(inputs_shape, filters, bias, CuDnnConv3dElemwise, \n",
    "                                                   inputs_theano_cudnn_contiguous)\n",
    "cost_cudnn = T.sum(conv_result_cudnn)\n",
    "cost_cudnn = gpu_contiguous(cost_cudnn)\n",
    "conv_dnn_gradient = T.grad(cost_cudnn, inputs_theano_cudnn)\n",
    "grad_func_cudnn = theano.function([inputs_theano_cudnn], conv_dnn_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "GpuDnn3dConvGradI: error doing operation: CUDNN_STATUS_NOT_SUPPORTED\nApply node that caused the error: GpuDnn3dConvGradI{inplace=False}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConv3dDesc{subsample=(1, 1, 1), conv_mode='cross'}.0, Constant{1.0}, Constant{0.0})\nInputs types: [CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, 5D), <theano.gof.type.CDataType object at 0x7feac0485cd0>, Scalar(float32), Scalar(float32)]\nInputs shapes: [(6, 3, 2, 3, 5), (5, 6, 1, 2, 3), (5, 3, 2, 4, 7), 'No shapes', (), ()]\nInputs strides: [(90, 30, 15, 5, 1), (36, 6, 0, 3, 1), (168, 56, 28, 7, 1), 'No strides', (), ()]\nInputs values: ['not shown', 'not shown', 'not shown', <PyCObject object at 0x7feadcfdfd78>, 1.0, 0.0]\n\nBacktrace when the node is created:\n  File \"/home/schirrmr/3dconv/pylearn3dconv/theano_dnn_first_try/theano_dnn_conv.py\", line 457, in make_node\n    def infer_shape(self, node, shape):\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-77a5709766a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcudnn_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_func_cudnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcudnn_result\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mcorrect_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/schirrmr/3dconv/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    606\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    609\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/schirrmr/3dconv/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: GpuDnn3dConvGradI: error doing operation: CUDNN_STATUS_NOT_SUPPORTED\nApply node that caused the error: GpuDnn3dConvGradI{inplace=False}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConv3dDesc{subsample=(1, 1, 1), conv_mode='cross'}.0, Constant{1.0}, Constant{0.0})\nInputs types: [CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, 5D), CudaNdarrayType(float32, 5D), <theano.gof.type.CDataType object at 0x7feac0485cd0>, Scalar(float32), Scalar(float32)]\nInputs shapes: [(6, 3, 2, 3, 5), (5, 6, 1, 2, 3), (5, 3, 2, 4, 7), 'No shapes', (), ()]\nInputs strides: [(90, 30, 15, 5, 1), (36, 6, 0, 3, 1), (168, 56, 28, 7, 1), 'No strides', (), ()]\nInputs values: ['not shown', 'not shown', 'not shown', <PyCObject object at 0x7feadcfdfd78>, 1.0, 0.0]\n\nBacktrace when the node is created:\n  File \"/home/schirrmr/3dconv/pylearn3dconv/theano_dnn_first_try/theano_dnn_conv.py\", line 457, in make_node\n    def infer_shape(self, node, shape):\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "cudnn_result = grad_func_cudnn(inputs)\n",
    "assert np.sum(np.square(cudnn_result- correct_result)) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuDnn 3d Theano "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano Memory Leak replication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory at start 2984.2 MB\n",
      "Total memory 3071.3 MB\n"
     ]
    }
   ],
   "source": [
    "import theano.sandbox.cuda\n",
    "print(\"Free memory at start {:5.1f} MB\".format(\n",
    "        theano.sandbox.cuda.mem_info()[0] / (1024.0 ** 2)))\n",
    "print(\"Total memory {:5.1f} MB\".format(\n",
    "        theano.sandbox.cuda.mem_info()[1] / (1024.0 ** 2)))\n",
    "# 2983.2 MB before import cell above\n",
    "# 2984.2 MB after import cell above -> all ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory at start 2975.2 MB\n"
     ]
    }
   ],
   "source": [
    "from pylearn3dconv.perf.perf_layers import generate_2d_3d_test_data\n",
    "rng = RandomState(hash('tobipuma') % 4294967295)\n",
    "inputs_shape = [12,20,20,14,3]\n",
    "filters_shape = [12,5,5,5,3]\n",
    "inputs, filters, bias, inputs_2d, filters_2d = generate_2d_3d_test_data(\n",
    "    rng, inputs_shape, filters_shape)   \n",
    "\n",
    "\n",
    "print(\"Free memory at start {:5.1f} MB\".format(\n",
    "        theano.sandbox.cuda.mem_info()[0] / (1024.0 ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (20, 20)\n",
      "Detector space: (16, 16)\n",
      "Output space: (16, 16)\n",
      "Free memory after compute reference result 2975.2 MB\n"
     ]
    }
   ],
   "source": [
    "from pylearn3dconv.perf.perf_layers import compute_2d_reference_result\n",
    "reference_result2d = compute_2d_reference_result(inputs_2d,filters_2d,bias)\n",
    "\n",
    "print(\"Free memory after compute reference result {:5.1f} MB\".format(\n",
    "        theano.sandbox.cuda.mem_info()[0] / (1024.0 ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory after del reference result 2975.2 MB\n"
     ]
    }
   ],
   "source": [
    "del inputs_2d\n",
    "del filters_2d\n",
    "del bias\n",
    "del reference_result2d\n",
    "print(\"Free memory after del reference result {:5.1f} MB\".format(\n",
    "        theano.sandbox.cuda.mem_info()[0] / (1024.0 ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Test Conv2d Layer Convolution replication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylearn3dconv.test_data import generate_test_data\n",
    "rng = RandomState(hash('tobipuma') % 4294967295)\n",
    "inputs_shape = [15, 6, 9, 11, 3]\n",
    "filters_shape = [12, 2, 4, 5, 3]\n",
    "inputs, filters, bias = generate_test_data(rng, inputs_shape, filters_shape)\n",
    "mlp = FakeMLP(rng=np.random,batch_size=inputs_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_fprop(conv_layer_class, inputs_shape, filters, bias, mlp):\n",
    "\n",
    "    conv_2d_layer = conv_layer_class(output_channels=filters.shape[0], \n",
    "        kernel_shape=filters.shape[1:3], tied_b=True,\n",
    "        layer_name='conv_lin', nonlinearity=IdentityConvNonlinearity(),\n",
    "        irange=0.001)\n",
    "    conv_2d_layer.set_mlp(mlp)\n",
    "    conv_2d_layer.set_input_space(Conv2DSpace(shape=inputs_shape[1:3], \n",
    "        num_channels=inputs_shape[4]))\n",
    "    converted_weights = Conv2DSpace.convert_numpy(filters[:,:,:,0,:], \n",
    "                                                  conv_2d_layer.input_space.axes, \n",
    "                                                  conv_2d_layer.detector_space.axes)\n",
    "\n",
    "    conv_2d_layer.set_weights(converted_weights)\n",
    "    conv_2d_layer.set_biases(bias)\n",
    "    inputs_2d_theano = T.ftensor4()\n",
    "    conv2d_result = conv_2d_layer.fprop(inputs_2d_theano)\n",
    "    conv2d = theano.function([inputs_2d_theano], conv2d_result)\n",
    "    return conv2d, conv_2d_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (6, 9)\n",
      "Detector space: (5, 6)\n",
      "Output space: (5, 6)\n"
     ]
    }
   ],
   "source": [
    "conv_2d, conv_2d_layer = create_fprop(ConvElemwise, inputs.shape, filters, bias, mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print np.sum(np.square(conv_2d_layer.get_biases()-conv_2d_blas_layer.get_biases()))\n",
    "# same!\n",
    "print np.sum(np.square(conv_2d_layer.get_params()[0].get_value()-\n",
    "                       conv_2d_blas_layer.get_params()[0].get_value()))\n",
    "# still same\n",
    "# so one is doing correlation other convolution...so flip filters???!?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.46139e-09\n"
     ]
    }
   ],
   "source": [
    "input_shape_2d = inputs.shape[0:3] + (inputs.shape[4],)\n",
    "conv_2d_blas, conv_2d_blas_layer = create_fprop(ConvElemwiseBlas, inputs.shape, filters[:,::-1,::-1,:,:], bias, mlp)\n",
    "\n",
    "result1= conv_2d(np.ones(input_shape_2d).astype(np.float32))\n",
    "result2= conv_2d_blas(np.ones(input_shape_2d).astype(np.float32))\n",
    "result1= conv_2d(inputs[:,:,:,0,:])\n",
    "result2= conv_2d_blas(inputs[:,:,:,0,:])\n",
    "\n",
    "print np.sum(np.square(result2-result1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Theano Cudnn Op 3d Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nreal_inputs = np.random.normal(size=(5,3,4,3,1)).astype(np.float32)\\nreal_filters = np.random.normal(size=(2,3,3,2,1)).astype(np.float32)\\nresult=conv_result_func(real_inputs, real_filters)\\nprint np.array(result)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs = ftensor5()\n",
    "filters = ftensor5()\n",
    "\n",
    "desc = owndnn.GpuDnnConv3dDesc(subsample=(1,1,1))()\n",
    "\n",
    "forward_conv = 1\n",
    "desc_op = desc.owner.op\n",
    "out_shp = owndnn.GpuDnn3dConv.get_out_shape(inputs.shape, filters.shape,\n",
    "                                   desc_op.subsample)\n",
    "\n",
    "out = gpu_alloc_empty(*out_shp)\n",
    "\n",
    "conv_result = owndnn.GpuDnn3dConv()(inputs, filters, out, desc)\n",
    "\n",
    "conv_result_func = theano.function([inputs, filters], conv_result, mode='DebugMode')\n",
    "\"\"\"\n",
    "real_inputs = np.random.normal(size=(5,3,4,3,1)).astype(np.float32)\n",
    "real_filters = np.random.normal(size=(2,3,3,2,1)).astype(np.float32)\n",
    "result=conv_result_func(real_inputs, real_filters)\n",
    "print np.array(result)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[[  1.3052038 ]\n",
      "    [  0.99738848]]\n",
      "\n",
      "   [[  1.23546088]\n",
      "    [ -3.78463674]]]\n",
      "\n",
      "\n",
      "  [[[  0.95624417]\n",
      "    [  3.17768621]]\n",
      "\n",
      "   [[  0.66182953]\n",
      "    [  4.60149765]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ -4.51677465]\n",
      "    [ -6.0400176 ]]\n",
      "\n",
      "   [[ 10.51604843]\n",
      "    [  5.51716518]]]\n",
      "\n",
      "\n",
      "  [[[ -1.98940551]\n",
      "    [  5.68342638]]\n",
      "\n",
      "   [[  2.21714044]\n",
      "    [ -1.55560589]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ -4.65615368]\n",
      "    [ -4.28107023]]\n",
      "\n",
      "   [[  5.39695978]\n",
      "    [  2.28894544]]]\n",
      "\n",
      "\n",
      "  [[[  4.28638983]\n",
      "    [ -4.13960791]]\n",
      "\n",
      "   [[  1.95399821]\n",
      "    [ -4.17457628]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ -5.53674126]\n",
      "    [ -6.08783388]]\n",
      "\n",
      "   [[  0.37000543]\n",
      "    [-12.15050125]]]\n",
      "\n",
      "\n",
      "  [[[  3.32547235]\n",
      "    [ -6.08842087]]\n",
      "\n",
      "   [[  7.22971964]\n",
      "    [  2.42332625]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[ -8.05591202]\n",
      "    [ 11.65596104]]\n",
      "\n",
      "   [[ -0.72672451]\n",
      "    [  1.53100789]]]\n",
      "\n",
      "\n",
      "  [[[ -5.61389637]\n",
      "    [  0.24655201]]\n",
      "\n",
      "   [[  7.72962809]\n",
      "    [  0.32530639]]]]]\n"
     ]
    }
   ],
   "source": [
    "real_inputs = np.random.normal(size=(5,3,4,3,1)).astype(np.float32)\n",
    "real_filters = np.random.normal(size=(2,3,3,2,1)).astype(np.float32)\n",
    "result=conv_result_func(real_inputs, real_filters)\n",
    "print np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for theano op convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = T.ftensor4()\n",
    "filters = T.ftensor4()\n",
    "\n",
    "desc = cdnn.GpuDnnConvDesc('valid')(inputs.shape, filters.shape)\n",
    "\n",
    "forward_conv = 1\n",
    "desc_op = desc.owner.op\n",
    "out_shp = cdnn.GpuDnnConv.get_out_shape(inputs.shape, filters.shape,\n",
    "                                   desc_op.border_mode,\n",
    "                                   desc_op.subsample)\n",
    "\n",
    "out = gpu_alloc_empty(*out_shp)\n",
    "\n",
    "conv_result = cdnn.GpuDnnConv()(inputs, filters, out, desc)\n",
    "\n",
    "conv_result_func = theano.function([inputs, filters], conv_result, mode='DebugMode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_inputs = np.random.normal(size=(5,3,4,1)).astype(np.float32)\n",
    "real_filters = np.random.normal(size=(2,3,3,1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-1.81879711]\n",
      "   [ 2.37883425]]\n",
      "\n",
      "  [[ 1.00554168]\n",
      "   [ 1.36972821]]]\n",
      "\n",
      "\n",
      " [[[-0.50348598]\n",
      "   [-1.81900835]]\n",
      "\n",
      "  [[-1.91241026]\n",
      "   [ 0.72988558]]]\n",
      "\n",
      "\n",
      " [[[-4.87137651]\n",
      "   [ 1.78511786]]\n",
      "\n",
      "  [[-4.22570562]\n",
      "   [ 3.57148552]]]\n",
      "\n",
      "\n",
      " [[[-0.41075268]\n",
      "   [ 0.05635083]]\n",
      "\n",
      "  [[ 0.44945204]\n",
      "   [ 4.61057091]]]\n",
      "\n",
      "\n",
      " [[[ 1.76157069]\n",
      "   [-4.53335381]]\n",
      "\n",
      "  [[-0.19954003]\n",
      "   [-5.59845734]]]]\n"
     ]
    }
   ],
   "source": [
    "result=conv_result_func(real_inputs, real_filters)\n",
    "print np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780\n"
     ]
    }
   ],
   "source": [
    "from pylearn3dconv.volumetric_space import Conv3DSpace\n",
    "from pylearn3dconv.layers.theano_3d_conv import Theano3dConv3dElemwise\n",
    "import numpy as np\n",
    "from pylearn2.models.mlp import IdentityConvNonlinearity\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from pylearn3dconv.volumetric_dense_design_matrix import VolumetricDenseDesignMatrix\n",
    "from pylearn2.training_algorithms.sgd import SGD\n",
    "from pylearn2.models.mlp import MLP, Softmax\n",
    "from pylearn2.format.target_format import OneHotFormatter\n",
    "from numpy.random import RandomState\n",
    "class FakeMLP():\n",
    "    def __init__(self,rng,batch_size):\n",
    "        self.rng = rng\n",
    "        self.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-0149da203385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfake_mlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFakeMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m conv_3d_layer = Theano3dConv3dElemwise(output_channels=filters_shape[0], \n\u001b[0;32m      4\u001b[0m         \u001b[0mkernel_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilters_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mlayer_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'conv3d_lin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonlinearity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mIdentityConvNonlinearity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rng' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "fake_mlp = FakeMLP(rng=rng,batch_size=inputs_shape[0])\n",
    "conv_3d_layer = Theano3dConv3dElemwise(output_channels=filters_shape[0], \n",
    "        kernel_shape=filters_shape[1:4],\n",
    "        layer_name='conv3d_lin', nonlinearity=IdentityConvNonlinearity(),\n",
    "        irange=0.001)\n",
    "conv_3d_layer.set_mlp(fake_mlp)\n",
    "conv_3d_input_space = Conv3DSpace(inputs_shape[1:4], num_channels=inputs_shape[4], axes=('b',0,1,2,'c'))\n",
    "conv_3d_layer.set_input_space(conv_3d_input_space)\n",
    "\n",
    "\n",
    "inputs_3d_layer_theano = ftensor5()\n",
    "conv3d__layer_result = conv_3d_layer.fprop(inputs_3d_layer_theano)\n",
    "conv3d_fprop = theano.function([inputs_3d_layer_theano], conv3d__layer_result)\n",
    "\n",
    "inputs_3d_nnet_theano = ftensor5()\n",
    "filters_3d_nnet_theano = ftensor5()\n",
    "bias_3d_nnet_theano = T.fvector()\n",
    "conv3d_nnet_result = theano.tensor.nnet.conv3D(inputs_3d_nnet_theano, filters_3d_nnet_theano, bias_3d_nnet_theano, d=(1,1,1))\n",
    "conv3d_nnet = theano.function([inputs_3d_nnet_theano, filters_3d_nnet_theano, bias_3d_nnet_theano], conv3d_nnet_result)\n",
    "\n",
    "inputs = (rng.rand(*inputs_shape).astype('float32') -1) * 2 # [-1,1)\n",
    "filters = (rng.rand(*filters_shape).astype('float32') - 1) *2 # [-1,1)\n",
    "bias = rng.rand(filters.shape[0]).astype('float32')\n",
    "targets = np.zeros(inputs.shape[0]).astype('int')\n",
    "targets[::2] = 1 # every second target is 1 or 0\n",
    "inputs[targets == 1] = inputs[targets == 1] + 1\n",
    "\n",
    "\n",
    "correct_result = conv3d_nnet(inputs, filters, bias)\n",
    "conv_3d_layer.set_weights(filters)\n",
    "conv_3d_layer.set_biases(bias)\n",
    "layer_result = conv3d_fprop(inputs)\n",
    "\n",
    "assert np.sum(np.square(correct_result - layer_result)) < 1e-4\n",
    "\n",
    "target_formatter = OneHotFormatter(2)\n",
    "targets_one_hot = target_formatter.format(targets)\n",
    "\n",
    "conv_3d_layer.mlp = None\n",
    "softmax_layer = Softmax(max_col_norm=2, layer_name='y', n_classes=2, istdev=.05)\n",
    "mlp = MLP(input_space=conv_3d_input_space, layers=[conv_3d_layer, softmax_layer])\n",
    "train_set = VolumetricDenseDesignMatrix(topo_view=inputs[0:50], y=targets_one_hot[0:50], axes=('b', 0, 1, 2, 'c'))\n",
    "valid_set = VolumetricDenseDesignMatrix(topo_view=inputs[50:75], y=targets_one_hot[50:75], axes=('b', 0, 1, 2, 'c'))\n",
    "test_set = VolumetricDenseDesignMatrix(topo_view=inputs[75:100], y=targets_one_hot[75:100], axes=('b', 0, 1, 2, 'c'))\n",
    "algorithm = SGD(batch_size=20, learning_rate=0.1)\n",
    "algorithm.setup(mlp, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('valid', 0.88)\n",
      "('test', 0.95999999999999996)\n"
     ]
    }
   ],
   "source": [
    "mlp_valid_result = mlp_fprop(valid_set.get_topological_view())\n",
    "mlp_valid_result_labels = np.argmax(mlp_valid_result, axis=1)\n",
    "mlp_test_result = mlp_fprop(test_set.get_topological_view())\n",
    "mlp_test_result_labels = np.argmax(mlp_test_result, axis=1)\n",
    "print(\"valid\", np.sum(np.equal(np.argmax(valid_set.y, axis=1), mlp_valid_result_labels)) / float(len(valid_set.y)))\n",
    "print(\"test\", np.sum(np.equal(np.argmax(test_set.y, axis=1), mlp_test_result_labels)) / float(len(test_set.y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29999999999999999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_mlp_theano = ftensor5()\n",
    "mlp_fprop_result = mlp.fprop(inputs_mlp_theano)\n",
    "\n",
    "mlp_fprop = theano.function([inputs_mlp_theano], mlp_fprop_result)\n",
    "mlp_result = mlp_fprop(train_set.get_topological_view())\n",
    "mlp_result_labels = np.argmax(mlp_result, axis=1)\n",
    "y_labels = np.argmax(train_set.y, axis=1)\n",
    "np.sum(np.equal(y_labels, mlp_result_labels)) / float(len(train_set.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "algorithm.train(train_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
